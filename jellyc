#!/usr/bin/env bash
#Deps parallel, aria2c, kde-desktop, ripgrep (rg)
#set -x
# Vars
regex_jellyfin_url_pattern="^https://[^/]+/Items/[a-z0-9]{32}/Download\?api_key=[a-z0-9]{32}"
max_clipboard_history_items_to_search=100
results_file="$HOME/Documents/jelly_dl_results.txt"
aria2_threads=1
aria2_instances=1

# Ensure the directory exists
mkdir -p "$(dirname "$results_file")"

# Ensure the file exists
touch "$results_file"

# Create a temporary file for download progress count
progress_count_file=$(mktemp)

# Initialize download progress count
echo 1 > "$progress_count_file"



# Function to print no URL message
print_no_url_message() {
    if [ "$clipboard_item_number" -lt "$max_clipboard_history_items_to_search" ]; then
        echo "Clipboard history ended after $clipboard_item_number items."
        echo "No valid URLs found within clipboard history items."
    else
        echo "No valid URLs found within $max_clipboard_history_items_to_search clipboard history items."
    fi
    exit 1
}



parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --help) 
                echo "Usage:"
                echo " -s num [ --split=num ] Number of threads to use on aria2 download"
                echo " -p num [ --parallel=num ] Number of simultaneous downloads spawned by parallel"
                exit 0 ;;
            -s) 
                shift
                aria2_threads=$1 ;;
            -p) 
                shift
                aria2_instances=$1 ;;
            --split=*)
                aria2_threads="${1/*=}" ;;
            --parallel=*)
                aria2_instances="${1/*=}" ;;
            *)
                echo "Unknown option: $1"
                exit 1 ;;
        esac
        shift
    done
    echo "aria2_threads is set to $aria2_threads. Change with option -s num"
    echo "aria2_instances is set to $aria2_instances. Change with option -p value"
    echo
}



# Function to search KDE clipboard history for urls matching regex jellyfin url pattern
search_clipboard() {
for (( clipboard_item_number = 0; clipboard_item_number < max_clipboard_history_items_to_search; clipboard_item_number++ )); do
    history_item=$(qdbus org.kde.klipper /klipper getClipboardHistoryItem $clipboard_item_number)
    if echo "$history_item" | grep -Pq "$regex_jellyfin_url_pattern"; then
        url_list_clipboard=$history_item
        echo "Matches found in clipboard history entry $clipboard_item_number:"
        echo "$url_list_clipboard"
        break
    fi
    # Break if history item is blank.
    if [ -z "$history_item" ]; then
        print_no_url_message $((i - 1))
    fi
done
}



# Function to convert URL list string to array
get_url_array() {
url_list_dirty="$url_list_clipboard"
SAVEIFS=$IFS          # Save current IFS (Internal Field Separator)
IFS=$'\n'             # Change IFS to newline char
url_list_dirty=($url_list_dirty)  # split the `url_list` string into an array by the same name
IFS=$SAVEIFS          # Restore original IFS
}



# Iterate over the array and add items to new array if they match the regex and not logged as downloaded in results_file
clean_url_array() {
    regex='^https://[^[:space:]]+$'
    url_list=()
    
    # Read the results file into a variable
    results_content=$(<"$results_file")

    for item in "${url_list_dirty[@]}"; do
        if [[ $item =~ $regex ]]; then
            escaped_url=$(echo "$item" | sed 's/[.[\*^$(){}?+|/]/\\&/g')
            pattern="Done: .*$escaped_url"
            
            # Search results file and add url to new array if not logged as downloaded
            if ! echo "$results_content" | rg -q "$pattern"; then
                url_list+=("$item")
            else
                echo "Already downloaded $item"
            fi
        fi
    done
    
    arraylength=${#url_list[@]}
}



set_dl_info() {
    # Dates and filenumber for log file
    datestamp=$(date +'%m/%d %H:%M:%S')
    padded_i=$(printf "%03d" $((i + 1)))

    while true; do
        # Retry mechanism for collecting filename
        max_attempts=5
        attempt=1
        filename=""

        while [[ -z "$filename" && $attempt -le $max_attempts ]]; do
            fileinfo="$(curl -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36" -i "$url" 2>/dev/null | head )"

            filename="$( echo "$fileinfo" | awk -F'filename=|filename\\*=UTF-8'\'' ' \
                '{ 
                    if ($2 ~ /^"/) {
                        match($2, /"([^"]+)"/, arr);
                        if (arr[1] != "") print arr[1];
                    } else {
                        match($2, /([^;]+)/, arr);
                        if (arr[1] != "") print arr[1];
                    }
                }' )"


            if [[ -z "$filename" ]]; then
                echo "Attempt $attempt/$max_attempts failed to collect filename."
                attempt=$((attempt + 1))
                sleep 1  # Optional: add a short delay between retries
            fi
        done

        if [[ -z "$filename" ]]; then
            echo "Error: Unable to collect filename after $max_attempts attempts. Sleeping for 300 seconds."
            sleep 300
        else
            # Format output filenames
            format_filename=$(echo "$filename" | perl -pe 's/(\{|\}|\[|\]|\s(?!$)|-(?![^\s]+$)|\(|\))+/./gm' | sed 's/"//g' )

            # Echo progress in style that matches aria2 colored progress output
            local lightgreen="\033[1;38;2;28;220;154m"
            local redish="\033[1;38;2;246;116;0m"
            local brightgreen="\033[1;38;2;17;209;22m"
            local reset_color="\033[0m"
            local notice="NOTICE"
            echo -e "$datestamp [${lightgreen}${notice}${reset_color}] Downloading ( ${brightgreen}$count${reset_color} of ${redish}$arraylength${reset_color} ) $format_filename"

            break
        fi
    done
}



aria2_dl() {
    echo "${format_filename}"
    aria2c \
      --allow-piece-length-change=true \
      --check-certificate=false \
      --continue=true \
      --max-concurrent-downloads=1 \
      --max-connection-per-server=$aria2_threads \
      --max-resume-failure-tries=64 \
      --max-file-not-found=64 \
      --max-tries=64 \
      --out="$format_filename" \
      --retry-wait=120 \
      --split=$aria2_threads \
      --timeout=10 \
      --user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36' \
      "$url"
}



download_urls() {
    # Read, increment, and write back the download progress count
    count=$(<"$progress_count_file")
    echo $((count + 1)) > "$progress_count_file"
    # Download URLs
    url="$1"
    set_dl_info
    aria2_dl

    if [ $? -eq 0 ]; then
        printf "$(date +"%Y/%m/%d %I:%M%p") $padded_i Done: $format_filename $url \n" >> "$results_file"
    else
        exit 0
    fi
}



# Function to handle Ctrl+C
handle_ctrl_c() {
    echo "Ctrl+C pressed. Aborting."
    exit 1
}



# Set up a trap to catch Ctrl+C and call handle_ctrl_c
trap handle_ctrl_c INT



main() {
    parse_arguments "$@"
    search_clipboard
    get_url_array
    clean_url_array

    if [[ $aria2_instances -eq 1 ]]; then
        for (( i=0; i<${#url_list[@]}; i++ )); do
            download_urls "${url_list[$i]}"  # Run download in the foreground
            
            # Check if MKV script is already running
            if ! pgrep -f "/home/jjenkx/.local/scripts/mkv" > /dev/null; then
                {
                    mkv_output=$( /home/jjenkx/.local/scripts/mkv )
                    echo
                    echo
                    echo
                    echo "$mkv_output"
                    echo
                    echo
                } &
            fi
        done
        wait  # Wait for all background MKV processes to finish
    else
        export -f aria2_dl
        export -f download_urls
        export -f set_dl_info
        export aria2_threads
        export arraylength
        export max_clipboard_history_items_to_search
        export progress_count_file
        export regex_jellyfin_url_pattern
        export results_file

        # Run downloads in parallel using GNU parallel
        printf "%s\n" "${url_list[@]}" | parallel -j $aria2_instances --lb download_urls &
        
        # Run the MKV script in the background, capturing its output
        {
            mkv_output=$( /home/jjenkx/.local/scripts/mkv )
            echo "$mkv_output"
        } &
        
        wait  # Wait for all background processes to finish
    fi
}

main "$@"


