#!/usr/bin/env bash
#Deps parallel, aria2c, kde-desktop, ripgrep (rg)

# Vars
regex_jellyfin_url_pattern="^https://[^/]+/Items/[a-z0-9]{32}/Download\?api_key=[a-z0-9]{32}"
max_clipboard_history_items_to_search=100
results_file="$HOME/Documents/jelly_dl_results.txt"
aria2_threads=1
aria2_instances=1

# Ensure the directory exists
mkdir -p "$(dirname "$results_file")"

# Ensure the file exists
touch "$results_file"

# Create a temporary file for download progress count
progress_count_file=$(mktemp)

# Initialize download progress count
echo 1 > "$progress_count_file"


# Function to print no URL message
print_no_url_message() {
    if [ "$clipboard_item_number" -lt "$max_clipboard_history_items_to_search" ]; then
        echo "Clipboard history ended after $clipboard_item_number items."
        echo "No valid URLs found within clipboard history items."
    else
        echo "No valid URLs found within $max_clipboard_history_items_to_search clipboard history items."
    fi
    exit 1
}


parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --help) 
                echo "Usage:"
                echo " -s num [ --split=num ] Number of threads to use on aria2 download"
                echo " -p num [ --parallel=num ] Number of simultaneous downloads spawned by parallel"
                exit 0 ;;
            -s) 
                shift
                aria2_threads=$1 ;;
            -p) 
                shift
                aria2_instances=$1 ;;
            --split=*)
                aria2_threads="${1/*=}" ;;
            --parallel=*)
                aria2_instances="${1/*=}" ;;
            *)
                echo "Unknown option: $1"
                exit 1 ;;
        esac
        shift
    done
    echo "aria2_threads is set to $aria2_threads. Change with option -s num"
    echo "aria2_instances is set to $aria2_instances. Change with option -p value"
    echo
}


# Function to search KDE clipboard history for urls matching regex jellyfin url pattern
search_clipboard() {
for (( clipboard_item_number = 0; clipboard_item_number < max_clipboard_history_items_to_search; clipboard_item_number++ )); do
    history_item=$(qdbus org.kde.klipper /klipper getClipboardHistoryItem $clipboard_item_number)
    if echo "$history_item" | grep -Pq "$regex_jellyfin_url_pattern"; then
        url_list_clipboard=$history_item
        echo "Matches found in clipboard history entry $clipboard_item_number:"
        echo "$url_list_clipboard"
        break
    fi
    # Break if history item is blank.
    if [ -z "$history_item" ]; then
        print_no_url_message $((i - 1))
    fi
done
}
    

# Function to convert URL list string to array
get_url_array() {
url_list_dirty="$url_list_clipboard"
SAVEIFS=$IFS          # Save current IFS (Internal Field Separator)
IFS=$'\n'             # Change IFS to newline char
url_list_dirty=($url_list_dirty)  # split the `url_list` string into an array by the same name
IFS=$SAVEIFS          # Restore original IFS
}


# Iterate over the array and add items to new array if they match the regex and not logged as downloaded in results_file
clean_url_array() {
    regex='^https://[^[:space:]]+$'
    url_list=()
    
    # Read the results file into a variable
    results_content=$(<"$results_file")

    for item in "${url_list_dirty[@]}"; do
        if [[ $item =~ $regex ]]; then
            escaped_url=$(echo "$item" | sed 's/[.[\*^$(){}?+|/]/\\&/g')
            pattern="Done: .*$escaped_url"
            
            # Search results file and add url to new array if not logged as downloaded
            if ! echo "$results_content" | rg -q "$pattern"; then
                url_list+=("$item")
            else
                echo "Already downloaded $item"
            fi
        fi
    done
    
    arraylength=${#url_list[@]}
}


set_dl_info() {
    # Dates and filenumber for log file
    datestamp=$(date +'%m/%d %H:%M:%S')
    padded_i=$(printf "%03d" $((i + 1)))

    # Format output filenames
    filename=$(curl -i "$url" 2>/dev/null | head | sed -n 's/.*filename="\([^"]*\)".*/\1/p' )
    format_filename=$(echo "$filename" | perl -pe 's/(\{|\}|\[|\]|\s(?!$)|-(?![^\s]+$)|\(|\))+/./gm' | sed 's/\.-/-/g' | sed 's/\.\+/\./g'
)

    # Echo progress in style that matches aria2 colored progress output
    local lightgreen="\033[1;38;2;28;220;154m"
    local redish="\033[1;38;2;246;116;0m"
    local brightgreen="\033[1;38;2;17;209;22m"
    local reset_color="\033[0m"
    local notice="NOTICE"
    echo -e "$datestamp [${lightgreen}${notice}${reset_color}] Downloading ( ${brightgreen}$count${reset_color} of ${redish}$arraylength${reset_color} ) $format_filename"
}


aria2_dl() {
    aria2c \
      --allow-piece-length-change=true \
      --check-certificate=false \
      --continue=true \
      --max-concurrent-downloads=1 \
      --max-connection-per-server=$aria2_threads \
      --max-resume-failure-tries=64 \
      --max-file-not-found=64 \
      --max-tries=64 \
      --out="$format_filename" \
      --retry-wait=120 \
      --split=$aria2_threads \
      --timeout=10 \
      --user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36' \
      "$url"
}
#      --allow-piece-length-change=true \
#      --check-certificate=false \
#      --console-log-level=notice \
#      --content-disposition-default-utf8=true \
#      --continue=true \
#      --disk-cache=1024M \
#      --download-result=full \
#      --file-allocation=falloc \
#      --enable-mmap=true \
#      --max-concurrent-downloads=1 \
#      --max-connection-per-server=$aria2_threads \
#      --max-mmap-limit=9223372036854775807 \
#      --max-resume-failure-tries=64 \
#      --max-file-not-found=64 \
#      --max-tries=64 \
#      --out="$format_filename" \
#      --realtime-chunk-checksum=true \
#      --retry-wait=120 \
#      --split=$aria2_threads \
#      --summary-interval=0 \
#      --timeout=10 \
#      --user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36' \


download_urls() {
    # Read, increment, and write back the download progress count
    count=$(<"$progress_count_file")
    echo $((count + 1)) > "$progress_count_file"

    # Download URLs
    url="$1"
    set_dl_info
    aria2_dl

    if [ $? -eq 0 ]; then
        printf "$(date +"%Y/%m/%d %I:%M%p") $padded_i Done: $format_filename $url \n" >> "$results_file"
    else
        exit 0
    fi
}



# Function to handle Ctrl+C
handle_ctrl_c() {
    echo "Ctrl+C pressed. Aborting."
    exit 1
}


# Set up a trap to catch Ctrl+C and call handle_ctrl_c
trap handle_ctrl_c INT


main() {
parse_arguments "$@"
search_clipboard
get_url_array
clean_url_array
if [[ $aria2_instances -eq 1 ]]; then
    for (( i=0; i<${#url_list[@]}; i++ )); do
        download_urls "${url_list[$i]}"
    done
else
    export -f aria2_dl
    export -f download_urls
    export -f set_dl_info
    export aria2_threads
    export arraylength
    export max_clipboard_history_items_to_search
    export progress_count_file
    export regex_jellyfin_url_pattern
    export results_file
    printf "%s\n" "${url_list[@]}" | parallel -j $aria2_instances --lb download_urls # | awk '{if (/^FILE:/) {print line1; print line2; print $0; getline; print $0; next;} line1=line2; line2=$0;}'
fi
}


main "$@"


/home/jjenkx/.local/scripts/mkv
